import numpy as np
import tensorflow as tf
import time
import pickle
from evaluator import ModelEvaluator
from model import BiLSTM_CNN, TemporalAttention, SpatialAttention
from bilstm import BiLSTM
from cnn import CNN
from gru import GRU
from ablation_models import SpatialOnlyModel, TemporalOnlyModel


def measure_detection_time(model, X_test):
    start_time = time.time()
    _ = model.predict(X_test)
    end_time = time.time()
    return (end_time - start_time) / len(X_test)


def print_model_results(name, metrics):
    print(f"\n=== {name} Results ===")
    print(f"Accuracy: {metrics['accuracy']:.4f}")
    print(f"F1-score: {metrics['f1']:.4f}")
    print(f"FPR: {metrics['fpr']:.4f}")
    print(f"FNR: {metrics['fnr']:.4f}")
    print(f"AUC: {metrics['auc']:.4f}")
    print(f"Running Time: {metrics['running_time']:.4f}s")

    print("\nConfusion Matrix:")
    print(metrics['confusion_matrix'])


def load_model_with_weights(name, model_class):
    """加载模型权重和配置"""
    # 创建模型实例
    model = model_class()
    # 加载权重
    model.load_weights(f'saved_models/{name}_weights.h5')
    return model


if __name__ == "__main__":
    current_time = "2025-04-28 03:29:09"
    current_user = "TIANQIISGOOD"
    print(f"Execution Time (UTC): {current_time}")
    print(f"User: {current_user}")
    print("=" * 50)

    # 加载测试数据
    print("\nLoading test data...")
    X_test = np.load('saved_models/X_test.npy')
    y_test = np.load('saved_models/y_test.npy')

    # 初始化评估器
    evaluator = ModelEvaluator()

    # 初始化结果字典
    results = {}

    # 模型类映射
    model_classes = {
        'BiLSTM-CNN': BiLSTM_CNN,
        'CNN': CNN,
        'BiLSTM': BiLSTM,
        'GRU': GRU,
        'Spatial-Only': SpatialOnlyModel,
        'Temporal-Only': TemporalOnlyModel
    }

    try:
        # 加载并评估每个模型
        for name, model_class in model_classes.items():
            print(f"\nLoading and evaluating {name} model...")

            # 加载模型和权重
            model = load_model_with_weights(name, model_class)

            # 评估模型
            results[name] = evaluator.evaluate(model, X_test, y_test)
            results[name]['detection_time'] = measure_detection_time(model, X_test)

            # 打印单个模型结果
            print_model_results(name, results[name])

        # 打印性能对比表
        print("\n=== Model Performance Comparison ===")
        print("{:<15} | {:<8} | {:<8} | {:<8} | {:<8} | {:<8} | {:<8}".format(
            "Model", "Accuracy", "F1", "FPR", "FNR", "AUC", "Time(s)"))
        print("-" * 80)
        for name, metrics in results.items():
            print("{:<15} | {:<8.4f} | {:<8.4f} | {:<8.4f} | {:<8.4f} | {:<8.4f} | {:<8.4f}".format(
                name,
                metrics['accuracy'],
                metrics['f1'],
                metrics['fpr'],
                metrics['fnr'],
                metrics['auc'],
                metrics['running_time']
            ))

        print("\n" + "=" * 50)
        print(f"Results generated at: {current_time} UTC")
        print(f"Generated by: {current_user}")

    except Exception as e:
        print(f"Error during testing: {str(e)}")
        print(f"Error occurred at: {current_time} UTC")
        print(f"User: {current_user}")