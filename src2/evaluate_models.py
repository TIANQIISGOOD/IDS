import numpy as np
import tensorflow as tf
import time
from evaluator import ModelEvaluator
from model import BiLSTM_CNN, TemporalAttention, SpatialAttention
from bilstm import BiLSTM
from cnn import CNN
from gru import GRU
from ablation_models import SpatialOnlyModel, TemporalOnlyModel


def measure_detection_time(model, X_test):
    start_time = time.time()
    _ = model.predict(X_test)
    end_time = time.time()
    return (end_time - start_time) / len(X_test)


def print_model_results(name, metrics):
    print(f"\n=== {name} Results ===")
    print(f"Accuracy: {metrics['accuracy']:.4f}")
    print(f"F1-score: {metrics['f1']:.4f}")
    print(f"FPR: {metrics['fpr']:.4f}")
    print(f"FNR: {metrics['fnr']:.4f}")
    print(f"AUC: {metrics['auc']:.4f}")
    print(f"Running Time: {metrics['running_time']:.4f}s")

    print("\nConfusion Matrix:")
    print(metrics['confusion_matrix'])


if __name__ == "__main__":
    current_time = "2025-04-28 03:34:28"
    current_user = "TIANQIISGOOD"
    print(f"Execution Time (UTC): {current_time}")
    print(f"User: {current_user}")
    print("=" * 50)

    # 加载测试数据
    print("\nLoading test data...")
    X_test = np.load('saved_models/X_test.npy')
    y_test = np.load('saved_models/y_test.npy')

    # 初始化评估器
    evaluator = ModelEvaluator()

    # 初始化结果字典
    results = {}

    # 自定义对象字典
    custom_objects = {
        'BiLSTM_CNN': BiLSTM_CNN,
        'TemporalAttention': TemporalAttention,
        'SpatialAttention': SpatialAttention,
        'BiLSTM': BiLSTM,
        'CNN': CNN,
        'GRU': GRU,
        'SpatialOnlyModel': SpatialOnlyModel,
        'TemporalOnlyModel': TemporalOnlyModel
    }

    try:
        # 加载并评估每个模型
        model_names = ['BiLSTM-CNN', 'CNN', 'BiLSTM', 'GRU', 'Spatial-Only', 'Temporal-Only']

        for name in model_names:
            print(f"\nLoading and evaluating {name} model...")

            # 使用自定义对象作用域加载模型
            with tf.keras.utils.custom_object_scope(custom_objects):
                model = tf.keras.models.load_model(f'saved_models/{name}_model')

            # 评估模型
            results[name] = evaluator.evaluate(model, X_test, y_test)
            results[name]['detection_time'] = measure_detection_time(model, X_test)

            # 打印单个模型结果
            print_model_results(name, results[name])

        # 打印性能对比表
        print("\n=== Model Performance Comparison ===")
        print("{:<15} | {:<8} | {:<8} | {:<8} | {:<8} | {:<8} | {:<8}".format(
            "Model", "Accuracy", "F1", "FPR", "FNR", "AUC", "Time(s)"))
        print("-" * 80)
        for name, metrics in results.items():
            print("{:<15} | {:<8.4f} | {:<8.4f} | {:<8.4f} | {:<8.4f} | {:<8.4f} | {:<8.4f}".format(
                name,
                metrics['accuracy'],
                metrics['f1'],
                metrics['fpr'],
                metrics['fnr'],
                metrics['auc'],
                metrics['running_time']
            ))

        print("\n" + "=" * 50)
        print(f"Results generated at: {current_time} UTC")
        print(f"Generated by: {current_user}")

    except Exception as e:
        import traceback

        print(f"Error during testing: {str(e)}")
        print("Detailed error:")
        print(traceback.format_exc())
        print(f"Error occurred at: {current_time} UTC")
        print(f"User: {current_user}")