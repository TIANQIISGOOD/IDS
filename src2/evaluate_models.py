import os
import numpy as np
import tensorflow as tf
import time
from evaluator import ModelEvaluator

# 获取当前脚本所在目录
current_dir = os.path.dirname(os.path.abspath(__file__))
saved_data_dir = os.path.join(current_dir, 'saved_data')
saved_models_dir = os.path.join(current_dir, 'saved_models')


def measure_detection_time(model, X_test):
    start_time = time.time()
    _ = model.predict(X_test)
    end_time = time.time()
    return (end_time - start_time) / len(X_test)


def print_model_results(name, metrics):
    print(f"\n=== {name} Results ===")
    print(f"Accuracy: {metrics['accuracy']:.4f}")
    print(f"F1-score: {metrics['f1']:.4f}")
    print(f"FPR: {metrics['fpr']:.4f}")
    print(f"FNR: {metrics['fnr']:.4f}")
    print(f"AUC: {metrics['auc']:.4f}")
    print(f"Running Time: {metrics['running_time']:.4f}s")

    print("\nConfusion Matrix:")
    print(metrics['confusion_matrix'])


if __name__ == "__main__":
    current_time = "2025-04-28 01:09:22"
    current_user = "TIANQIISGOOD"
    print(f"Execution Time (UTC): {current_time}")
    print(f"User: {current_user}")
    print("=" * 50)

    # 加载评估数据
    print("\nLoading evaluation data...")
    try:
        X_val = np.load(os.path.join(saved_data_dir, 'X_val.npy'))
        y_val = np.load(os.path.join(saved_data_dir, 'y_val.npy'))
        X_test = np.load(os.path.join(saved_data_dir, 'X_test.npy'))
        y_test = np.load(os.path.join(saved_data_dir, 'y_test.npy'))
        print("Data loaded successfully!")
    except Exception as e:
        print(f"Error loading data: {e}")
        raise

    # 初始化评估器和结果字典
    evaluator = ModelEvaluator()
    results = {}

    try:
        # 加载并评估所有模型
        model_names = ['BiLSTM-CNN', 'CNN', 'BiLSTM', 'GRU', 'Spatial-Only', 'Temporal-Only']

        for model_name in model_names:
            print(f"\nEvaluating {model_name} model...")
            model_path = os.path.join(saved_models_dir, f'{model_name}.h5')
            model = tf.keras.models.load_model(model_path)

            # 在测试集上评估
            results[model_name] = evaluator.evaluate(model, X_test, y_test)
            results[model_name]['detection_time'] = measure_detection_time(model, X_test)

            # 打印结果
            print_model_results(model_name, results[model_name])

        # 打印性能对比表
        print("\n=== Model Performance Comparison ===")
        print("{:<15} | {:<8} | {:<8} | {:<8} | {:<8} | {:<8} | {:<8}".format(
            "Model", "Accuracy", "F1", "FPR", "FNR", "AUC", "Time(s)"))
        print("-" * 80)
        for name, metrics in results.items():
            print("{:<15} | {:<8.4f} | {:<8.4f} | {:<8.4f} | {:<8.4f} | {:<8.4f} | {:<8.4f}".format(
                name,
                metrics['accuracy'],
                metrics['f1'],
                metrics['fpr'],
                metrics['fnr'],
                metrics['auc'],
                metrics['running_time']
            ))

        print("\n" + "=" * 50)
        print(f"Results generated at: {current_time} UTC")
        print(f"Generated by: {current_user}")

    except Exception as e:
        print(f"评估过程中发生错误: {str(e)}")
        print(f"Error occurred at: {current_time} UTC")
        print(f"User: {current_user}")